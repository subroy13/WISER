{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac5d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from typing import Any, Union\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from llm_utilities import get_torch_device, generate_llm_tokens, unwatermarked_token_generation\n",
    "from watermarking_func import (\n",
    "    gumbel_token_generation, pivot_statistic_gumbel_func,\n",
    "    inverse_token_generation, pivot_statistic_inverse_func,\n",
    "    pf_token_generation, pivot_statistic_pf_func,\n",
    "    redgreen_token_generation, pivot_statistic_redgreen_func,\n",
    "    synthid_token_generation, pivot_statistic_synthid_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e9887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mapes family of Effingham enjoy the Lincoln Park Zoo in Chicago with their children including their adopted children, Regino and Regina, who were born in the Philippines.\n",
      "Misty Mapes and her husband, Patrick, of Effingham always had a desire to add to their family through adoption.\n",
      "That dream became a reality in part due to Gift of Adoption Fund, a nonprofit organization that provides financial support to families that need help to pay for the hefty cost of adopting a child.\n",
      "The Mapes, who h\n"
     ]
    }
   ],
   "source": [
    "root_data_path = \"../data\"\n",
    "output_data_path = \"../data/output\"\n",
    "\n",
    "# Read the list of prompts\n",
    "def get_prompts():\n",
    "    with open(os.path.join(root_data_path, \"prompts_subset.txt\"), \"r\", errors=\"ignore\") as f:\n",
    "        prompts = f.read().split(\"\\n===\\n\")\n",
    "        f.close()\n",
    "    return prompts\n",
    "\n",
    "def normalize_name(name: str):\n",
    "    name = re.sub(r'[^A-Za-z0-9]', '-', name)\n",
    "    name = re.sub(r'-+', '-', name)\n",
    "    return name\n",
    "\n",
    "\n",
    "prompt_list = get_prompts()\n",
    "print(prompt_list[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f1d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_watermarked_data(\n",
    "    model_name: str,\n",
    "    token_generation_func: dict,\n",
    "    pivot_func: Any = None,\n",
    "    device: Any = None,\n",
    "    output_filename: Union[str, None] = None,\n",
    "    prompt_tokens: int = 50,\n",
    "    output_tokens: int = 200,\n",
    "    batch_size: int = 8,\n",
    "    max_token_input_length: int = 256,\n",
    "    initial_seed: int = 1234\n",
    "):\n",
    "    if device is None:\n",
    "        device = get_torch_device(force_cpu=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device) # type: ignore\n",
    "    vocab_size = model.get_output_embeddings().weight.shape[0]\n",
    "    print(f\"There are {vocab_size} many words in vocabulary\")\n",
    "    print(f\"The model {model_name} is loaded on device: {device}\")\n",
    "\n",
    "    # calculate the intervals\n",
    "    intervals = []\n",
    "    last_interval_type = None\n",
    "    last_interval_index = None\n",
    "    data_gen_type = \"unwatermarked\"\n",
    "    for index in sorted([int(x) for x in token_generation_func.keys()], reverse = False):\n",
    "        if last_interval_type is not None:\n",
    "            intervals.append((last_interval_index, index, last_interval_type))\n",
    "        last_interval_type = token_generation_func[str(index)].__name__.split('_')[0]\n",
    "        if last_interval_type != \"unwatermarked\":\n",
    "            data_gen_type = last_interval_type\n",
    "        last_interval_index = index\n",
    "    intervals.append((last_interval_index, output_tokens, last_interval_type))\n",
    "\n",
    "    data_out_conf = {\n",
    "        \"model_name\": model_name,\n",
    "        \"intervals\": intervals,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"out_tokens\": output_tokens,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"initial_seed\": initial_seed,\n",
    "        \"max_token_input_length\": max_token_input_length\n",
    "    }\n",
    "    if output_filename is None:\n",
    "        output_filename = f\"data_{normalize_name(model_name)}_n{output_tokens}_{data_gen_type}.json\"\n",
    "    \n",
    "    response_list = []\n",
    "    pivot_seed = initial_seed + prompt_tokens \n",
    "    prompt_list = get_prompts()\n",
    "    for i in tqdm(range(0, len(prompt_list), batch_size), desc=\"Processing batches\"):\n",
    "        prompt_batch = prompt_list[i:(i+batch_size)]\n",
    "        response = generate_llm_tokens(\n",
    "            prompt_batch,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            token_generation_func=token_generation_func,\n",
    "            verbose=False,\n",
    "            out_tokens=output_tokens,\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            vocab_size=vocab_size,\n",
    "            max_token_input_length=max_token_input_length,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        if pivot_func is not None:\n",
    "            # calculate pivot function as well\n",
    "            for j in range(len(response)):\n",
    "                gen_tokens = response[j][\"gen_tokens\"]\n",
    "                response[j][\"pivots\"] = pivot_func(gen_tokens, seed = pivot_seed, vocab_size = vocab_size)\n",
    "        response_list.extend(response)\n",
    "\n",
    "        # save the json file\n",
    "        with open(os.path.join(output_data_path, output_filename), \"w\") as f:\n",
    "            json.dump({\"configuration\": data_out_conf, \"data\": response_list}, f)\n",
    "            f.close()\n",
    "\n",
    "    # save it at last as well\n",
    "    with open(os.path.join(output_data_path, output_filename), \"w\") as f:\n",
    "        json.dump({\"configuration\": data_out_conf, \"data\": response_list}, f)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fc4615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 262144 many words in vocabulary\n",
      "The model google/gemma-3-270m is loaded on device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e4a5d0dc9b4947b4b91c32bcb65e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = get_torch_device()\n",
    "\n",
    "output_tokens = 500\n",
    "model_name = \"google/gemma-3-270m\"\n",
    "\n",
    "def strong_redgreen_token_generation(probs, counter, vocab_size):\n",
    "    return redgreen_token_generation(probs, counter, vocab_size, delta = 2.0)\n",
    "\n",
    "def weak_redgreen_token_generation(probs, counter, vocab_size):\n",
    "    return redgreen_token_generation(probs, counter, vocab_size, delta = 1.0)\n",
    "\n",
    "\n",
    "# generate data for strong but short segment\n",
    "token_generation_func = {\n",
    "    \"0\": unwatermarked_token_generation,\n",
    "    \"50\": weak_redgreen_token_generation,\n",
    "    \"450\": unwatermarked_token_generation\n",
    "}\n",
    "pivot_func = pivot_statistic_redgreen_func\n",
    "\n",
    "generate_watermarked_data(\n",
    "    model_name,\n",
    "    token_generation_func,\n",
    "    pivot_func,\n",
    "    output_tokens=output_tokens,\n",
    "    device = device,\n",
    "    batch_size=8,\n",
    "    output_filename=f\"data_{normalize_name(model_name)}_weak_but_long.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b5ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
