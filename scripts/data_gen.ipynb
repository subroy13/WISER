{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9fecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm utility codes\n",
    "\n",
    "# generate llm text without watermarking\n",
    "def unwatermarked_token_generation(probs, counter, vocab_size):\n",
    "    gen_tokens = torch.multinomial(probs, 1)\n",
    "    return gen_tokens\n",
    "\n",
    "#############\n",
    "# GUMBEL Watermarking\n",
    "\n",
    "# generate llm text with gumbel watermarking\n",
    "def gumbel_token_generation(probs: torch.Tensor, counter, vocab_size, seed=1234):\n",
    "    device = probs.device\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed + counter)\n",
    "    unif_noise = torch.rand(vocab_size, generator=g).to(device)\n",
    "    gumbel_ratio = torch.log(unif_noise) / probs[0]\n",
    "    return torch.argmax(gumbel_ratio).view(-1, 1)\n",
    "\n",
    "\n",
    "def pivot_statistic_gumbel_func(gen_tokens, vocab_size, seed=1234):\n",
    "    # gen_tokens is a numpy array, so convert into torch Tensor for torch operations\n",
    "    pivot_stat = []\n",
    "    for counter, gen_token in enumerate(gen_tokens):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed + counter)\n",
    "        unif_noise = torch.rand(vocab_size, generator=g)\n",
    "        pivot_stat.append(-torch.log(1 - unif_noise[gen_token]).item())\n",
    "    return pivot_stat\n",
    "\n",
    "\n",
    "######################\n",
    "# Inverse Watermarking\n",
    "\n",
    "# generate llm text with inverse watermarking\n",
    "def inverse_token_generation(probs: torch.Tensor, counter, vocab_size, seed=1234):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed + counter)\n",
    "    unif_noise = torch.rand(1, generator=g)  # (1,)\n",
    "    pi = torch.randperm(vocab_size, generator=g)  # random permutation (vocab_size, )\n",
    "    inv_pi = torch.empty_like(pi)\n",
    "    inv_pi[pi] = torch.arange(vocab_size)\n",
    "\n",
    "    probs_shuffled = probs[0, inv_pi]  # probs is shape (1, vocab_size)\n",
    "    cdf = torch.cumsum(probs_shuffled, dim=0)  # (vocab_size,)\n",
    "    index = torch.searchsorted(\n",
    "        cdf, unif_noise.item(), right=False\n",
    "    )  # Find the first index where cdf exceeds unif_noise\n",
    "\n",
    "    # Return the original vocab index corresponding to the sampled one\n",
    "    return inv_pi[index].view(-1, 1)\n",
    "\n",
    "\n",
    "def pivot_statistic_inverse_func(gen_tokens, vocab_size, seed=1234):\n",
    "    pivot_stat = []\n",
    "    for counter, gen_token in enumerate(gen_tokens):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed + counter)\n",
    "        unif_noise = torch.rand(1, generator=g)  # (1,)\n",
    "        pi = torch.randperm(vocab_size, generator=g)  # random permutation (vocab_size, )\n",
    "        normalized = pi[gen_token] / (vocab_size - 1) # as pi[gen_token] yields a value between 0 to (vocab_size - 1)\n",
    "        pivot_stat.append(1 - np.abs((normalized - unif_noise).item()))  # 1 - <..> so that under H0, mean is small\n",
    "    return pivot_stat\n",
    "\n",
    "\n",
    "# Common function that can be used to generate tokens using LLM\n",
    "def generate_llm_tokens(\n",
    "    prompt: str,\n",
    "    tokenizer: PreTrainedTokenizer,  # usually AutoTokenizer\n",
    "    model,  # usually AutoModelForCausalLM\n",
    "    token_generation_func,  # a token generation function, or a dict <start_index>:<token_gen_func>, see below.\n",
    "    verbose=False,\n",
    "    prompt_tokens=50,  # take the first 50 tokens of prompt as input\n",
    "    out_tokens=50,  # output next 50 tokens\n",
    "    vocab_size=None,\n",
    "):\n",
    "    # It is also possible to provide input to the token_generation_func a dictionary of the following form\n",
    "    # {\n",
    "    #     \"0\": watermark_func_1,\n",
    "    #     \"t1\": watermark_func_2,\n",
    "    #     \"t2\": watermark_func_3,\n",
    "    #     ...\n",
    "    # }\n",
    "    # It allows to use different watermarking scheme to be added in between\n",
    "    if vocab_size is None or vocab_size < 0:\n",
    "      vocab_size = model.get_output_embeddings().weight.shape[0]\n",
    "\n",
    "    tokens = tokenizer.encode(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=2000\n",
    "    )  # get the token vector\n",
    "    torch_prompt = tokens[:, :prompt_tokens]  # give the first prompt_tokens as input\n",
    "    inputs = torch_prompt.to(model.device)\n",
    "    counter_range = tqdm(range(out_tokens)) if verbose else range(out_tokens)\n",
    "    gen_tokens = []\n",
    "    for counter in counter_range:\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)  # apply the model\n",
    "        probs = torch.nn.functional.softmax(\n",
    "            output.logits[:, -1, :], dim=-1\n",
    "        )  # apply softmax over the last dimension\n",
    "\n",
    "        # extract the token generation function\n",
    "        if isinstance(token_generation_func, dict):\n",
    "            token_change_times = [int(x) for x in list(token_generation_func.keys())]\n",
    "            for key in sorted(token_change_times, reverse=True):\n",
    "                if key <= counter:\n",
    "                    token_gen_func = token_generation_func[str(key)]\n",
    "                    break\n",
    "        else:\n",
    "            token_change_times = []  # no change times\n",
    "            token_gen_func = token_generation_func\n",
    "\n",
    "        gen_token = token_gen_func(\n",
    "            probs, counter + prompt_tokens, vocab_size\n",
    "        )  # calculate the token\n",
    "\n",
    "        gen_token = gen_token.to(model.device)  # move to the device\n",
    "        gen_tokens.append(gen_token.item())  # append to gen_tokens as numpy array\n",
    "        inputs = torch.concat(\n",
    "            (inputs, gen_token), dim=1\n",
    "        )  # first dim = batch_size, second dim needs to merge\n",
    "\n",
    "    # at the end, produce the decoded text\n",
    "    out_text = tokenizer.decode(inputs[0])\n",
    "    return {\n",
    "        \"prompt\": tokenizer.decode(torch_prompt[0]),\n",
    "        \"gen_tokens\": gen_tokens,\n",
    "        \"output\": out_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e25e1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mapes family of Effingham enjoy the Lincoln Park Zoo in Chicago with their children including their adopted children, Regino and Regina, who were born in the Philippines.\n",
      "Misty Mapes and her husband, Patrick, of Effingham always had a desire to add to their family through adoption.\n",
      "That dream became a reality in part due to Gift of Adoption Fund, a nonprofit organization that provides financial support to families that need help to pay for the hefty cost of adopting a child.\n",
      "The Mapes, who have two biological children, Braydon, 16, and Madison, 11, were able to adopt 8-year-old twins, Regina and Regino — who go by Ina and Ino — about a year ago from the Philippines. They received a $4,000 grant that helped them pay travel expenses to the Philippines to bring the children home.\n",
      "Though they had always had tossed around the idea of adoption, they were spurred to take action when their older son asked them about it a few years ago.\n",
      "\"He said, 'Hey. Can we adopt? I'd like to have a brother,\" Misty Mapes recalled.\n",
      "Misty Mapes, who works as a teacher, and Patrick, who is a dockworker, set about eliminating as many of their expenses as they could to save the $40,000 they would need to took to fund the adoption.\n",
      "That final boost was provided by the Gift of Adoption Fund, which bridges the game when people are nearing the goal of completing the adoption, but need a little extra financial help to finalize it, said Marcy McKay, a La Grange resident and volunteer for the fund.\n",
      "Though they didn't need the financial help to adopt their children that the Gift of Adoption Fund provides, the McKays, who have three adopted children ages 14, 13, and 11, know how costly it is and are working to help other families adopt.\n",
      "In November 2014, Bethany and Jared Crain got a call saying that they were matched with an expectant mother.\n",
      "While they were ecstatic that they could be adding to their family of three, they decided not to tell their 4-year-old that she could be getting a sibling.\n",
      "\"It's just so extremely expensive,\" said McKay. \"A lot of people don't have that kind of money.\"\n",
      "The need on the part of children to have families is great, too. The fund estimates there are 140 million children around the world who are orphaned and 500,000 in the U.S. who are living in foster care and have no permanent family to call their own.\n",
      "The goal of the fund is to inspire adoption by providing grants to qualified parents. Parents who seek grants are required to show that they have financial need and have already completed some of the steps needed to adopt such as working with a licensed agency and having a home study done.\n",
      "The fund puts an emphasis on completing adoptions for children who may find it more difficult to be placed with a permanent family. Those children may be siblings, like Regina and Regino Mapes, children who are aging out, have medical needs or who may go into foster care.\n",
      "The grants that are supplied by the nonprofit range from $3,500 to $7,500.\n",
      "Joan Schoon's desire to be a foster parent was born out of compassion and sympathy.\n",
      "As a teenager, she remembers listening to her friends who were foster children complain about treatment they endured in some of their previous foster homes.\n",
      "\"They apply for the exact dollar amount they need and the grants are paid directly to the adoption agency,\" McKay said.\n",
      "The fund was founded in 1996 by a couple who, like the McKays, felt thankful that they had the financial resources to pay for adopting children, and wanted to help other families who need the support, according to its website.\n",
      "She said the group raises money through a variety of fundraisers such as cocktail parties and golf outings.\n",
      "\"It's such a compelling cause,\" said McKay, noting that many of their donations are in the $50 to $100 ranges.\n",
      "\"It's a perfect fit,\" she said.\n"
     ]
    }
   ],
   "source": [
    "root_data_path = \"../data\"\n",
    "output_data_path = \"../data/output\"\n",
    "\n",
    "def get_prompts():\n",
    "    with open(os.path.join(root_data_path, \"prompts_subset.txt\"), \"r\", errors=\"ignore\") as f:\n",
    "        prompts = f.read().split(\"\\n===\\n\")\n",
    "    return prompts\n",
    "\n",
    "prompt_list = get_prompts()\n",
    "print(prompt_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fff8f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50272 many words in vocabulary\n",
      "The model facebook/opt-125m is loaded on device: cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "vocab_size = model.get_output_embeddings().weight.shape[0]\n",
    "print(f\"There are {vocab_size} many words in vocabulary\")\n",
    "print(f\"The model {model_name} is loaded on device: {device}\")\n",
    "\n",
    "torch.set_num_threads(8) # set only 8 threads to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79683338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data configuration to run for\n",
    "data_configuration = {\n",
    "        \"fname\": \"data_inverse_n500.json\",\n",
    "        \"prompt_tokens\": 50,\n",
    "        \"out_tokens\": 500,\n",
    "        \"token_generation_func\": {\n",
    "            \"0\": unwatermarked_token_generation,\n",
    "            \"100\": inverse_token_generation,\n",
    "            \"200\": unwatermarked_token_generation,\n",
    "            \"400\": inverse_token_generation,\n",
    "            \"450\": unwatermarked_token_generation,\n",
    "        },\n",
    "        \"pivot\": pivot_statistic_inverse_func\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d48cc88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accebf5fca0340be86848b5434459e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the main simulation loop\n",
    "prompt_tokens: int = data_configuration.get(\"prompt_tokens\", 0)\n",
    "out_tokens: int = data_configuration.get(\"out_tokens\", 0)\n",
    "pivot_func = data_configuration.get(\"pivot\")\n",
    "pivot_seed = 1234 + prompt_tokens  # this is where the seed for pivot statistic will start from    \n",
    "token_generation_func_serialized = {\n",
    "    k: v.__name__ for k, v in data_configuration.get(\"token_generation_func\", {}).items()\n",
    "}\n",
    "    \n",
    "data_list = []\n",
    "save_every = 10\n",
    "counter = 0\n",
    "for prompt in tqdm(prompt_list):\n",
    "    counter += 1\n",
    "    response = generate_llm_tokens(\n",
    "        prompt,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        token_generation_func=data_configuration.get(\"token_generation_func\", {}),\n",
    "        verbose=False,\n",
    "        out_tokens=out_tokens,\n",
    "        prompt_tokens=prompt_tokens\n",
    "    )\n",
    "    if pivot_func is not None:\n",
    "        # calculate pivot function as well\n",
    "        gen_tokens = response[\"gen_tokens\"]\n",
    "        response[\"pivots\"] = pivot_func(gen_tokens, seed = pivot_seed, vocab_size = vocab_size)\n",
    "    data_list.append(response)\n",
    "\n",
    "    if counter % save_every == 0:\n",
    "        # save the JSON\n",
    "        data_outfile = data_configuration.get(\"fname\", \"data.json\")\n",
    "        with open(os.path.join(output_data_path, data_outfile), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"configuration\": {\n",
    "                    \"token_generation_func\": token_generation_func_serialized,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"prompt_tokens\": prompt_tokens,\n",
    "                    \"out_tokens\": out_tokens,\n",
    "                    \"vocab_size\": vocab_size\n",
    "                },\n",
    "                \"data\": data_list\n",
    "            }, f)\n",
    "            f.close()\n",
    "\n",
    "\n",
    "# save the JSON at last as well\n",
    "data_outfile = data_configuration.get(\"fname\", \"data.json\")\n",
    "with open(os.path.join(output_data_path, data_outfile), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"configuration\": {\n",
    "            \"token_generation_func\": token_generation_func_serialized,\n",
    "            \"model_name\": model_name,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"out_tokens\": out_tokens,\n",
    "            \"vocab_size\": vocab_size\n",
    "        },\n",
    "        \"data\": data_list\n",
    "    }, f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fab987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
