{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ddd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizer\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b522d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e2178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mapes family of Effingham enjoy the Lincoln Park Zoo in Chicago with their children including their adopted children, Regino and Regina, who were born in the Philippines.\n",
      "Misty Mapes and her husband, Patrick, of Effingham always had a desire to add to their family through adoption.\n",
      "That dream became a reality in part due to Gift of Adoption Fund, a nonprofit organization that provides financial support to families that need help to pay for the hefty cost of adopting a child.\n",
      "The Mapes, who h\n"
     ]
    }
   ],
   "source": [
    "root_data_path = \"../data\"\n",
    "output_data_path = \"../data/output\"\n",
    "\n",
    "def get_prompts():\n",
    "    with open(os.path.join(root_data_path, \"prompts_subset.txt\"), \"r\", errors=\"ignore\") as f:\n",
    "        prompts = f.read().split(\"\\n===\\n\")\n",
    "    return prompts\n",
    "\n",
    "prompt_list = get_prompts()\n",
    "print(prompt_list[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6088dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f84b3eaf0e4ad1b8c05093e9007f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e56fabc1c2443cbb6a13272bc04c364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59909daaa1524d328b50c7bf749e96fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2b1502f08a49bfb199a239b8f45da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719e6f2c63a247308b2744211b1bac36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056d048622b54a05a71c1dae395822f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04aa5a688cd94780bb60444f7a1454b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50272 many words in vocabulary\n",
      "The model facebook/opt-125m is loaded on device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed5902ca3c5449eb10544aecf5118d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "# model_name = \"google/gemma-3-270m\"\n",
    "# model_name = \"openai-community/gpt2\"\n",
    "# model_name = \"EleutherAI/gpt-neo-125m\"\n",
    "\n",
    "# Get pytorch device\n",
    "def get_torch_device(force_cpu: bool = False):\n",
    "    if force_cpu:\n",
    "        device_name = \"cpu\"\n",
    "    elif torch.cuda.is_available():\n",
    "        device_name = \"cuda:0\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_name = \"mps\"\n",
    "    else:\n",
    "        device_name = \"cpu\"\n",
    "    return torch.device(device_name)\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "device = get_torch_device(force_cpu=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "vocab_size = model.get_output_embeddings().weight.shape[0]\n",
    "print(f\"There are {vocab_size} many words in vocabulary\")\n",
    "print(f\"The model {model_name} is loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d491be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate llm text without watermarking\n",
    "def unwatermarked_token_generation(probs, counter, vocab_size, seed = 1234):\n",
    "    g = torch.Generator(device = probs.device)\n",
    "    g.manual_seed(seed + counter)\n",
    "    gen_tokens = torch.multinomial(probs, 1, generator=g)\n",
    "    return gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e4d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Gumbel watermarking\n",
    "def gumbel_token_generation(probs: torch.Tensor, counter, vocab_size, seed=1234):\n",
    "    device = probs.device\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed + counter)\n",
    "    unif_noise = torch.rand(vocab_size, generator=g).to(device)\n",
    "    gumbel_ratio = torch.log(unif_noise) / probs[0]\n",
    "    return torch.argmax(gumbel_ratio).view(-1, 1)\n",
    "\n",
    "# pivot statistic\n",
    "def pivot_statistic_gumbel_func(gen_tokens, vocab_size, seed=1234):\n",
    "    # gen_tokens is a numpy array, so convert into torch Tensor for torch operations\n",
    "    pivot_stat = []\n",
    "    for counter, gen_token in enumerate(gen_tokens):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed + counter)\n",
    "        unif_noise = torch.rand(vocab_size, generator=g)\n",
    "        pivot_stat.append(-torch.log(1 - unif_noise[gen_token]).item())\n",
    "    return pivot_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6b2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Inverse Watermarking\n",
    "\n",
    "# generate llm text with inverse watermarking\n",
    "def inverse_token_generation(probs: torch.Tensor, counter, vocab_size, seed=1234):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed + counter)\n",
    "    unif_noise = torch.rand(1, generator=g)  # (1,)\n",
    "    pi = torch.randperm(vocab_size, generator=g)  # random permutation (vocab_size, )\n",
    "    inv_pi = torch.empty_like(pi)\n",
    "    inv_pi[pi] = torch.arange(vocab_size)\n",
    "\n",
    "    probs_shuffled = probs[0, inv_pi]  # probs is shape (1, vocab_size)\n",
    "    cdf = torch.cumsum(probs_shuffled, dim=0)  # (vocab_size,)\n",
    "    index = torch.searchsorted(\n",
    "        cdf, unif_noise.item(), right=False\n",
    "    )  # Find the first index where cdf exceeds unif_noise\n",
    "\n",
    "    # Return the original vocab index corresponding to the sampled one\n",
    "    return inv_pi[index].view(-1, 1)\n",
    "\n",
    "\n",
    "def pivot_statistic_inverse_func(gen_tokens, vocab_size, seed=1234):\n",
    "    pivot_stat = []\n",
    "    for counter, gen_token in enumerate(gen_tokens):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed + counter)\n",
    "        unif_noise = torch.rand(1, generator=g)  # (1,)\n",
    "        pi = torch.randperm(vocab_size, generator=g)  # random permutation (vocab_size, )\n",
    "        normalized = pi[gen_token] / (vocab_size - 1) # as pi[gen_token] yields a value between 0 to (vocab_size - 1)\n",
    "        pivot_stat.append(1 - np.abs((normalized - unif_noise).item()))  # 1 - <..> so that under H0, mean is small\n",
    "    return pivot_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4251fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utility functions for generating llm texts\n",
    "def generate_llm_tokens(\n",
    "    prompts: list[str],\n",
    "    tokenizer,  # usually AutoTokenizer\n",
    "    model,  # usually AutoModelForCausalLM\n",
    "    token_generation_func: Any,  # a token generation function, or a dict <start_index>:<token_gen_func>, see below.\n",
    "    verbose=False,\n",
    "    prompt_tokens=50,  # take the first 50 tokens of prompt as input\n",
    "    out_tokens=50,  # output next 50 tokens\n",
    "    vocab_size=None,\n",
    "    batch_size = 8,\n",
    "    max_token_input_length = 256\n",
    "):\n",
    "    # It is also possible to provide input to the token_generation_func a dictionary of the following form\n",
    "    # {\n",
    "    #     \"0\": watermark_func_1,\n",
    "    #     \"t1\": watermark_func_2,\n",
    "    #     \"t2\": watermark_func_3,\n",
    "    #     ...\n",
    "    # }\n",
    "    # It allows to use different watermarking scheme to be added in between\n",
    "    if vocab_size is None or vocab_size < 0:\n",
    "        vocab_size = model.get_output_embeddings().weight.shape[0]\n",
    "\n",
    "    # some preparation\n",
    "    if isinstance(token_generation_func, dict):\n",
    "        token_change_times = [int(x) for x in list(token_generation_func.keys())]\n",
    "        token_change_times = sorted(token_change_times, reverse=True)\n",
    "    else:\n",
    "        token_change_times = []\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        prompts[:batch_size],\n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    torch_prompt = tokens['input_ids'][:, :prompt_tokens]\n",
    "    inputs = torch_prompt.to(model.device)\n",
    "    inputs_to_decode = inputs\n",
    "    counter_range = tqdm(range(out_tokens)) if verbose else range(out_tokens)\n",
    "\n",
    "    gen_tokens = []\n",
    "    past = None\n",
    "    for counter in counter_range:\n",
    "        with torch.no_grad():\n",
    "            if past:\n",
    "                output = model(inputs[:,-1:], past_key_values = past)  # apply the model\n",
    "            else:\n",
    "                output = model(inputs)\n",
    "        probs = torch.nn.functional.softmax(output.logits[:, -1, :], dim = 1)  # apply softmax over the last dimension\n",
    "        past = output.past_key_values\n",
    "\n",
    "        # extract the token generation function\n",
    "        if len(token_change_times) > 0:\n",
    "            for key in token_change_times:\n",
    "                if key <= counter:\n",
    "                    token_gen_func : Any = token_generation_func[str(key)]\n",
    "                    break\n",
    "        else:\n",
    "            token_gen_func : Any = token_generation_func\n",
    "\n",
    "        # for each row in batch, run the token generation function\n",
    "        gen_token_indices = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            gen_token = token_gen_func(\n",
    "                probs = probs[i, :].view(1, -1), \n",
    "                counter=counter + prompt_tokens, \n",
    "                vocab_size = vocab_size\n",
    "            ) # calculate the token\n",
    "            gen_token_indices.append(int(gen_token.item()))\n",
    "\n",
    "        gen_tokens.append(gen_token_indices) # shape = (out_tokens, batch_size)\n",
    "        gen_token_indices = torch.tensor(gen_token_indices, dtype = inputs.dtype, device=model.device).view(-1, 1) # shape = (batch_size, 1)\n",
    "        inputs = torch.concat((inputs, gen_token_indices), dim = 1) # keep first dim as it is, merge across 2nd dim\n",
    "        inputs_to_decode = torch.concat((inputs_to_decode, gen_token_indices), dim = 1) # this is complete token sequence\n",
    "\n",
    "        # subset to max size\n",
    "        if inputs.shape[1] > max_token_input_length:\n",
    "            inputs = inputs[:, -max_token_input_length:]\n",
    "\n",
    "    # at the end, produce the decoded text\n",
    "    out_text_list = tokenizer.batch_decode(inputs_to_decode)\n",
    "    input_text_list = tokenizer.batch_decode(torch_prompt)\n",
    "    return [{\n",
    "        \"prompt\": input_text_list[i],\n",
    "        \"gen_tokens\": np.array(gen_tokens)[:, i].tolist(),\n",
    "        \"output\": out_text_list[i]\n",
    "    } for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d11c66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data for a specific configuration\n",
    "batch_size = 8\n",
    "max_token_input_length = 256\n",
    "\n",
    "prompt_tokens = 50\n",
    "output_tokens = 500\n",
    "output_filename = \"data_inverse_n500_facebook_opt125m.json\"\n",
    "token_generation_func = {\n",
    "    \"0\": unwatermarked_token_generation,\n",
    "    \"100\": inverse_token_generation,\n",
    "    \"200\": unwatermarked_token_generation,\n",
    "    \"400\": inverse_token_generation,\n",
    "    \"450\": unwatermarked_token_generation,\n",
    "}\n",
    "pivot_func = pivot_statistic_inverse_func\n",
    "# pivot_func = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e584ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f26013432e4c3d92ba0fdd0c2cf342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the main simulation loop\n",
    "pivot_seed = 1234 + prompt_tokens  # this is where the seed for pivot statistic will start from\n",
    "batch_size = 8\n",
    "max_token_input_length = 256\n",
    "\n",
    "intervals = []\n",
    "last_interval_type = None\n",
    "last_interval_index = None\n",
    "\n",
    "# calculate the intervals\n",
    "for index in sorted([int(x) for x in token_generation_func.keys()], reverse = False):\n",
    "    if last_interval_type is not None:\n",
    "        intervals.append((last_interval_index, index, last_interval_type))\n",
    "    last_interval_type = token_generation_func[str(index)].__name__.split('_')[0]\n",
    "    last_interval_index = index\n",
    "intervals.append((last_interval_index, output_tokens, last_interval_type))\n",
    "\n",
    "data_out_conf = {\n",
    "    \"model_name\": model_name,\n",
    "    \"intervals\": intervals,\n",
    "    \"prompt_tokens\": prompt_tokens,\n",
    "    \"out_tokens\": output_tokens,\n",
    "    \"vocab_size\": vocab_size\n",
    "}\n",
    "\n",
    "response_list = []\n",
    "for i in tqdm(range(0, len(prompt_list), batch_size), desc=\"Processing batches\"):\n",
    "    prompt_batch = prompt_list[i:(i+batch_size)]\n",
    "    response = generate_llm_tokens(\n",
    "        prompt_batch,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        token_generation_func=token_generation_func,\n",
    "        verbose=False,\n",
    "        out_tokens=output_tokens,\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        vocab_size=vocab_size,\n",
    "        max_token_input_length=max_token_input_length\n",
    "    )\n",
    "    if pivot_func is not None:\n",
    "        # calculate pivot function as well\n",
    "        for j in range(len(response)):\n",
    "            gen_tokens = response[j][\"gen_tokens\"]\n",
    "            response[j][\"pivots\"] = pivot_func(gen_tokens, seed = pivot_seed, vocab_size = vocab_size)\n",
    "    response_list.extend(response)\n",
    "\n",
    "    # save the json file\n",
    "    with open(os.path.join(output_data_path, output_filename), \"w\") as f:\n",
    "        json.dump({\"configuration\": data_out_conf, \"data\": response_list}, f)\n",
    "        f.close()\n",
    "\n",
    "# save it at last as well\n",
    "with open(os.path.join(output_data_path, output_filename), \"w\") as f:\n",
    "    json.dump({\"configuration\": data_out_conf, \"data\": response_list}, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd0692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
