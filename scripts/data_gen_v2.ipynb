{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ddd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b522d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e2178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mapes family of Effingham enjoy the Lincoln Park Zoo in Chicago with their children including their adopted children, Regino and Regina, who were born in the Philippines.\n",
      "Misty Mapes and her husband, Patrick, of Effingham always had a desire to add to their family through adoption.\n",
      "That dream became a reality in part due to Gift of Adoption Fund, a nonprofit organization that provides financial support to families that need help to pay for the hefty cost of adopting a child.\n",
      "The Mapes, who h\n"
     ]
    }
   ],
   "source": [
    "root_data_path = \"../data\"\n",
    "output_data_path = \"../data/output_v2\"\n",
    "\n",
    "def get_prompts():\n",
    "    with open(os.path.join(root_data_path, \"prompts_subset.txt\"), \"r\", errors=\"ignore\") as f:\n",
    "        prompts = f.read().split(\"\\n===\\n\")\n",
    "    return prompts\n",
    "\n",
    "prompt_list = get_prompts()\n",
    "print(prompt_list[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4251fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utility functions for generating llm texts\n",
    "def generate_llm_tokens(\n",
    "    prompts: list[str],\n",
    "    tokenizer,  # usually AutoTokenizer\n",
    "    model,  # usually AutoModelForCausalLM\n",
    "    token_generation_func,  # a token generation function, or a dict <start_index>:<token_gen_func>, see below.\n",
    "    verbose=False,\n",
    "    prompt_tokens=50,  # take the first 50 tokens of prompt as input\n",
    "    out_tokens=50,  # output next 50 tokens\n",
    "    vocab_size=None,\n",
    "    batch_size = 8,\n",
    "    max_token_input_length = 256\n",
    "):\n",
    "    # It is also possible to provide input to the token_generation_func a dictionary of the following form\n",
    "    # {\n",
    "    #     \"0\": watermark_func_1,\n",
    "    #     \"t1\": watermark_func_2,\n",
    "    #     \"t2\": watermark_func_3,\n",
    "    #     ...\n",
    "    # }\n",
    "    # It allows to use different watermarking scheme to be added in between\n",
    "    if vocab_size is None or vocab_size < 0:\n",
    "        vocab_size = model.get_output_embeddings().weight.shape[0]\n",
    "\n",
    "    # some preparation\n",
    "    if isinstance(token_generation_func, dict):\n",
    "        token_change_times = [int(x) for x in list(token_generation_func.keys())]\n",
    "        token_change_times = sorted(token_change_times, reverse=True)\n",
    "    else:\n",
    "        token_change_times = []\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        prompts[:batch_size],\n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    torch_prompt = tokens['input_ids'][:, :prompt_tokens]\n",
    "    inputs = torch_prompt.to(model.device)\n",
    "    inputs_to_decode = inputs\n",
    "    counter_range = tqdm(range(out_tokens)) if verbose else range(out_tokens)\n",
    "\n",
    "    gen_tokens = []\n",
    "    for counter in counter_range:\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)  # apply the model\n",
    "        probs = torch.nn.functional.softmax(output.logits[:, -1, :], dim = 1)  # apply softmax over the last dimension\n",
    "\n",
    "        # extract the token generation function\n",
    "        if len(token_change_times) > 0:\n",
    "            for key in token_change_times:\n",
    "                if key <= counter:\n",
    "                    token_gen_func = token_generation_func[str(key)]\n",
    "                    break\n",
    "        else:\n",
    "            token_gen_func = token_generation_func\n",
    "\n",
    "        # for each row in batch, run the token generation function\n",
    "        gen_token_indices = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            gen_token = token_gen_func(\n",
    "                probs = probs[i, :].view(1, -1), \n",
    "                counter=counter + prompt_tokens, \n",
    "                vocab_size = vocab_size\n",
    "            ) # calculate the token\n",
    "            gen_token_indices.append(int(gen_token.item()))\n",
    "\n",
    "        gen_tokens.append(gen_token_indices) # shape = (out_tokens, batch_size)\n",
    "        gen_token_indices = torch.tensor(gen_token_indices, dtype = inputs.dtype, device=model.device).view(-1, 1) # shape = (batch_size, 1)\n",
    "        inputs = torch.concat((inputs, gen_token_indices), dim = 1) # keep first dim as it is, merge across 2nd dim\n",
    "        inputs_to_decode = torch.concat((inputs_to_decode, gen_token_indices), dim = 1) # this is complete token sequence\n",
    "\n",
    "        # subset to max size\n",
    "        if inputs.shape[1] > max_token_input_length:\n",
    "            inputs = inputs[:, -max_token_input_length:]\n",
    "\n",
    "\n",
    "    # at the end, produce the decoded text\n",
    "    out_text_list = tokenizer.batch_decode(inputs_to_decode)\n",
    "    input_text_list = tokenizer.batch_decode(torch_prompt)\n",
    "    return [{\n",
    "        \"prompt\": input_text_list[i],\n",
    "        \"gen_tokens\": np.array(gen_tokens)[:, i],\n",
    "        \"output\": out_text_list[i]\n",
    "    } for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11c66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate llm text without watermarking\n",
    "def unwatermarked_token_generation(probs, counter, vocab_size):\n",
    "    gen_tokens = torch.multinomial(probs, 1)\n",
    "    return gen_tokens\n",
    "\n",
    "#############\n",
    "# IMPORTANT: These are not to be batched as then the seeding does not work!\n",
    "# GUMBEL Watermarking\n",
    "\n",
    "# generate llm text with gumbel watermarking\n",
    "def gumbel_token_generation(probs: torch.Tensor, counter, vocab_size, seed=1234):\n",
    "    device = probs.device\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed + counter)\n",
    "    unif_noise = torch.rand(vocab_size, generator=g).to(device)\n",
    "    gumbel_ratio = torch.log(unif_noise) / probs[0]\n",
    "    return torch.argmax(gumbel_ratio).view(-1, 1)\n",
    "\n",
    "\n",
    "def pivot_statistic_gumbel_func(gen_tokens, vocab_size, seed=1234):\n",
    "    # gen_tokens is a numpy array, so convert into torch Tensor for torch operations\n",
    "    pivot_stat = []\n",
    "    for counter, gen_token in enumerate(gen_tokens):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed + counter)\n",
    "        unif_noise = torch.rand(vocab_size, generator=g)\n",
    "        pivot_stat.append(-torch.log(1 - unif_noise[gen_token]).item())\n",
    "    return pivot_stat\n",
    "\n",
    "\n",
    "######################\n",
    "# Inverse Watermarking\n",
    "\n",
    "# generate llm text with inverse watermarking\n",
    "def inverse_token_generation(probs: torch.Tensor, counter, vocab_size, seed=1234):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed + counter)\n",
    "    unif_noise = torch.rand(1, generator=g)  # (1,)\n",
    "    pi = torch.randperm(vocab_size, generator=g)  # random permutation (vocab_size, )\n",
    "    inv_pi = torch.empty_like(pi)\n",
    "    inv_pi[pi] = torch.arange(vocab_size)\n",
    "\n",
    "    probs_shuffled = probs[0, inv_pi]  # probs is shape (1, vocab_size)\n",
    "    cdf = torch.cumsum(probs_shuffled, dim=0)  # (vocab_size,)\n",
    "    index = torch.searchsorted(\n",
    "        cdf, unif_noise.item(), right=False\n",
    "    )  # Find the first index where cdf exceeds unif_noise\n",
    "\n",
    "    # Return the original vocab index corresponding to the sampled one\n",
    "    return inv_pi[index].view(-1, 1)\n",
    "\n",
    "\n",
    "def pivot_statistic_inverse_func(gen_tokens, vocab_size, seed=1234):\n",
    "    pivot_stat = []\n",
    "    for counter, gen_token in enumerate(gen_tokens):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed + counter)\n",
    "        unif_noise = torch.rand(1, generator=g)  # (1,)\n",
    "        pi = torch.randperm(vocab_size, generator=g)  # random permutation (vocab_size, )\n",
    "        normalized = pi[gen_token] / (vocab_size - 1) # as pi[gen_token] yields a value between 0 to (vocab_size - 1)\n",
    "        pivot_stat.append(1 - np.abs((normalized - unif_noise).item()))  # 1 - <..> so that under H0, mean is small\n",
    "    return pivot_stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284dded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50272 many words in vocabulary\n",
      "The model facebook/opt-125m is loaded on device: cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "vocab_size = model.get_output_embeddings().weight.shape[0]\n",
    "print(f\"There are {vocab_size} many words in vocabulary\")\n",
    "print(f\"The model {model_name} is loaded on device: {device}\")\n",
    "\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "513959dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7c99c37102438c8f15b5d04dc15f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the data configuration to run for\n",
    "batch_size = 8\n",
    "max_token_input_length = 8\n",
    "\n",
    "data_configuration = {\n",
    "    \"fname\": \"data_inverse_n500.json\",\n",
    "    \"prompt_tokens\": 50,\n",
    "    \"out_tokens\": 500,\n",
    "    \"token_generation_func\": {\n",
    "        \"0\": unwatermarked_token_generation,\n",
    "        \"100\": inverse_token_generation,\n",
    "        \"200\": unwatermarked_token_generation,\n",
    "        \"400\": inverse_token_generation,\n",
    "        \"450\": unwatermarked_token_generation,\n",
    "    },\n",
    "    \"pivot\": pivot_statistic_inverse_func\n",
    "}\n",
    "\n",
    "# Run the main simulation loop\n",
    "prompt_tokens: int = data_configuration.get(\"prompt_tokens\", 0)\n",
    "out_tokens: int = data_configuration.get(\"out_tokens\", 0)\n",
    "pivot_func = data_configuration.get(\"pivot\")\n",
    "pivot_seed = 1234 + prompt_tokens  # this is where the seed for pivot statistic will start from    \n",
    "token_generation_func_serialized = {\n",
    "    k: v.__name__ for k, v in data_configuration.get(\"token_generation_func\", {}).items()\n",
    "}\n",
    "\n",
    "response = generate_llm_tokens(\n",
    "    prompt_list[:batch_size],\n",
    "    tokenizer,\n",
    "    model,\n",
    "    token_generation_func=data_configuration.get(\"token_generation_func\", {}),\n",
    "    verbose=True,\n",
    "    out_tokens=out_tokens,\n",
    "    prompt_tokens=prompt_tokens,\n",
    "    max_token_input_length=max_token_input_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a740b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>The Mapes family of Effingham enjoy the Lincoln Park Zoo in Chicago with their children including their adopted children, Regino and Regina, who were born in the Philippines.\n",
      "Misty Mapes and her husband, Patrick, of Effingham\n",
      "\n",
      "\n",
      "</s>The Mapes family of Effingham enjoy the Lincoln Park Zoo in Chicago with their children including their adopted children, Regino and Regina, who were born in the Philippines.\n",
      "Misty Mapes and her husband, Patrick, of Effingham purchased the Buckner Mitarrie for $25,000,000 and a 35,877-acre property on Pederson used as a sexual assault counselor there, Janov was on her way to becoming a NGE co-Leader.\n",
      "\n",
      "Naux owns 280 properties totaling approximately 3.1 billion rupees. (Reporting by Sudarshan Varadharan that brought internet moments of his own. Impressed with the American-Israeli relationship, and they enforce compliance with trade secret laws and other international treaties concerning \"Hobservirri pe diskut cu BB-ul timpului de teme, iar, aste resignat prin bagajul din oradea 23 de ani necesita sa facem să ascuma cum spuneam, am când a putea really să acționeze inst structură picia de 70 de servicii de clientes boala de monturi argovinda nosotne oznasi custougos vropolis cameraworx venezolano, photos of our grandkids prominently decorated on security H&M jackets and pants made it to the top-end of the market research report. The report contains vast insights into the revenue and volume growth forecasts for the Department of Health, Business and Commonwealth of the Commonwealth of Australia claimants during the recent Gulf War\n",
      "DI (i.e., the ten team playoff bracket indicates two alternate teams went to 1/2 full point and to only be used when throwing out\n",
      "the third and longer dangerion to the 3 and 7-Sinologist system and apparatus to make synthesis iterative of main\n",
      "windows in tandem with multiple blades. is theophers 1975 Spotify interview, this time with a guest Tanner McSherry in his delivery of letters reminding him that the father has been faithful to his daughter. Their mortgage was but one similarity, utterly jealous of Covid Mills: reports\n",
      "At the moment, no further updates were published yesterday.\n",
      "\n",
      "Advertisement:\n",
      "\n",
      "What the Obama administration took away with the Romney tax.\n",
      "Related: Massive GOP tax hike could hit the presidency\"\n",
      "\n",
      "The prime minister said the growth of the corporate sector was achieved in 2017-2018, and the number was specified as 1 at time of publication. The identification and reason for its existence involved: the fact that his memories and feelings made him to want to fulfil our hassles too much and doesn’t belong in\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(response[i]['prompt'] + \"\\n\\n\")\n",
    "print(response[i]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd0692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
